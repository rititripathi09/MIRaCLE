{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXF8YYmR+tIwuCNnSVtd/R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rititripathi09/MIRaCLE/blob/main/CLMIR(2025)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN-01"
      ],
      "metadata": {
        "id": "fY7lj1Eo8wWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy sentence-transformers sympy faiss-cpu\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import sympy as sp\n",
        "import re\n",
        "import warnings\n",
        "import torch\n",
        "import faiss\n",
        "import os\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "print(\"Pandas imported successfully, version:\", pd.__version__)\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to mount Google Drive: {e}\")\n",
        "\n",
        "#MODEL INITALIZING IS DONE OVER HERE\n",
        "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "expected_dim = model.get_sentence_embedding_dimension()\n",
        "print(f\"Model expected dimension: {expected_dim}\")\n",
        "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "train_data_path = '/content/drive/MyDrive/clmir/indexed_documents.csv'\n",
        "test_data_path = '/content/drive/MyDrive/clmir/indexed_test_data.csv'\n",
        "train_embeddings_path = '/content/drive/MyDrive/clmir/document_embeddings.csv'\n",
        "output_path = '/content/drive/MyDrive/clmir/retrieval_results.csv'\n",
        "\n",
        "# DEBUUGING OF THE FILE PATHS\n",
        "print(f\"Train data path: {train_data_path}, Type: {type(train_data_path)}\")\n",
        "print(f\"Test data path: {test_data_path}, Type: {type(test_data_path)}\")\n",
        "print(f\"Train embeddings path: {train_embeddings_path}, Type: {type(train_embeddings_path)}\")\n",
        "print(f\"Output path: {output_path}, Type: {type(output_path)}\")\n",
        "\n",
        "# VALIDATION OF THE FILE PATH TYPES- CORRECT OR NOT- IF NOT GIVE ERROR\n",
        "if not isinstance(train_data_path, (str, bytes, os.PathLike)):\n",
        "    raise TypeError(f\"train_data_path must be a string or PathLike, got {type(train_data_path)}\")\n",
        "if not isinstance(test_data_path, (str, bytes, os.PathLike)):\n",
        "    raise TypeError(f\"test_data_path must be a string or PathLike, got {type(test_data_path)}\")\n",
        "\n",
        "# CHECXKS IF I/O FILE IS THER/CORRECT OR NOT\n",
        "if not os.path.exists(train_data_path):\n",
        "    raise FileNotFoundError(f\"Training file not found at {train_data_path}. Ensure the file exists and Google Drive is mounted.\")\n",
        "if not os.path.exists(test_data_path):\n",
        "    raise FileNotFoundError(f\"Test file not found at {test_data_path}. Ensure the file exists and Google Drive is mounted.\")\n",
        "\n",
        "# DATA LOADING AND PRINING COLUMN ADN THERE SIZES FOR DEBUGGING AND CHECK COLUMN AND TEST SET SIZE\n",
        "start_time = time.time()\n",
        "try:\n",
        "    train_df = pd.read_csv(train_data_path)\n",
        "    test_df = pd.read_csv(test_data_path)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error loading CSV files: {e}\")\n",
        "print(f\"load_data took {time.time() - start_time:.2f} seconds\")\n",
        "print(\"Training data columns:\", train_df.columns.tolist())\n",
        "print(\"Test data columns:\", test_df.columns.tolist())\n",
        "print(f\"Training data size: {len(train_df)} documents\")\n",
        "print(f\"Test data size: {len(test_df)} queries\")\n",
        "if 'docno' not in train_df.columns or 'text' not in train_df.columns:\n",
        "    raise KeyError(\"Training data must contain 'docno' and 'text' columns\")\n",
        "if 'qid' not in test_df.columns or 'query' not in test_df.columns:\n",
        "    raise KeyError(\"Test data must contain 'qid' and 'query' columns\")\n",
        "if len(test_df) != 50:\n",
        "    raise ValueError(f\"Test set must contain exactly 50 queries, got {len(test_df)}\")\n",
        "\n",
        "# CLEAING OF LATEX AS MATHEMATICAL EXPRESSIONS OF THERE ANY\n",
        "def clean_latex(expr):\n",
        "    expr = re.sub(r'\\\\[a-zA-Z]+', '', expr)\n",
        "    expr = expr.replace('{', '').replace('}', '')\n",
        "    expr = expr.replace('^', '**')\n",
        "    expr = re.sub(r'\\s+', '', expr)\n",
        "    return expr\n",
        "\n",
        "# EXTRACTION OF TEXT AND MATHEMATICAL EXPRESSION\n",
        "def extract_text_math(content):\n",
        "    math_pattern = r'\\$.*?\\$|[\\w\\^*/+\\-()=]+|\\sum.*?\\)|integral.*?\\)|partial.*?\\)|limit.*?\\)'\n",
        "    math_expressions = [clean_latex(expr) for expr in re.findall(math_pattern, str(content))][:1]  # Limit to top 1 expression\n",
        "    clean_text = re.sub(math_pattern, '', str(content)).strip()\n",
        "    return clean_text, math_expressions\n",
        "\n",
        "# Csymp lib used. math similarity-text to mathematical expression\n",
        "math_cache = {}\n",
        "def cached_sympify(expr):\n",
        "    if expr not in math_cache:\n",
        "        try:\n",
        "            math_cache[expr] = sp.sympify(expr, evaluate=False)\n",
        "        except:\n",
        "            math_cache[expr] = None\n",
        "    return math_cache[expr]\n",
        "\n",
        "# compute math expression similarity\n",
        "def math_similarity(expr1, expr2):\n",
        "    try:\n",
        "        parsed_expr1 = cached_sympify(expr1)\n",
        "        parsed_expr2 = cached_sympify(expr2)\n",
        "        if parsed_expr1 is None or parsed_expr2 is None:\n",
        "            return 0.0\n",
        "        if str(parsed_expr1) == str(parsed_expr2):\n",
        "            return 1.0\n",
        "        if parsed_expr1 == parsed_expr2:\n",
        "            return 1.0\n",
        "        diff = sp.simplify(parsed_expr1 - parsed_expr2)\n",
        "        if diff == 0:\n",
        "            return 1.0\n",
        "        return 0.5\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "# Preprocessing of the data\n",
        "start_time = time.time()\n",
        "train_df['clean_text'], train_df['math_expressions'] = zip(*train_df['text'].apply(extract_text_math))\n",
        "test_df['clean_text'], test_df['math_expressions'] = zip(*test_df['query'].apply(extract_text_math))\n",
        "train_df['clean_text'] = train_df['clean_text'].replace('', ' ').fillna(' ')\n",
        "test_df['clean_text'] = test_df['clean_text'].replace('', ' ').fillna(' ')\n",
        "print(f\"preprocess_data took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Load or compute train embeddings, either use the exisited or generate it\n",
        "start_time = time.time()\n",
        "batch_size = 128 if torch.cuda.is_available() else 64\n",
        "if os.path.exists(train_embeddings_path):\n",
        "    print(\"Loading precomputed train embeddings...\")\n",
        "    embeddings_df = pd.read_csv(train_embeddings_path)\n",
        "    print(f\"Precomputed embeddings CSV shape: {embeddings_df.shape}\")\n",
        "    if embeddings_df.shape[1] != expected_dim:\n",
        "        print(f\"Warning: Precomputed embeddings dimension {embeddings_df.shape[1]} does not match model dimension {expected_dim}. Recomputing embeddings...\")\n",
        "        train_text_embeddings = model.encode(train_df['clean_text'].tolist(), batch_size=batch_size, convert_to_tensor=True, show_progress_bar=True)\n",
        "        pd.DataFrame(train_text_embeddings.cpu().numpy()).to_csv(train_embeddings_path, index=False)\n",
        "        print(f\"Saved new train embeddings to {train_embeddings_path}, shape: {train_text_embeddings.shape}\")\n",
        "    else:\n",
        "        train_text_embeddings = torch.tensor(embeddings_df.values, dtype=torch.float32).to(model.device)\n",
        "        print(f\"Loaded train embeddings shape: {train_text_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"Computing train embeddings...\")\n",
        "    train_text_embeddings = model.encode(train_df['clean_text'].tolist(), batch_size=batch_size, convert_to_tensor=True, show_progress_bar=True)\n",
        "    pd.DataFrame(train_text_embeddings.cpu().numpy()).to_csv(train_embeddings_path, index=False)\n",
        "    print(f\"Saved new train embeddings to {train_embeddings_path}, shape: {train_text_embeddings.shape}\")\n",
        "\n",
        "test_text_embeddings = model.encode(test_df['clean_text'].tolist(), batch_size=batch_size, convert_to_tensor=True, show_progress_bar=True)\n",
        "print(f\"Test embeddings shape: {test_text_embeddings.shape}\")\n",
        "print(f\"load_or_compute_embeddings took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# dimensionality verification for the embeddings dimensions\n",
        "if train_text_embeddings.shape[1] != expected_dim:\n",
        "    raise ValueError(f\"Train embeddings dimension {train_text_embeddings.shape[1]} does not match model dimension {expected_dim}\")\n",
        "if test_text_embeddings.shape[1] != expected_dim:\n",
        "    raise ValueError(f\"Test embeddings dimension {test_text_embeddings.shape[1]} does not match model dimension {expected_dim}\")\n",
        "\n",
        "# Building FAISS index with IVF optimization\n",
        "start_time = time.time()\n",
        "dimension = train_text_embeddings.shape[1]\n",
        "nlist = 100\n",
        "quantizer = faiss.IndexFlatL2(dimension)\n",
        "index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
        "train_embeddings_np = np.ascontiguousarray(train_text_embeddings.cpu().numpy(), dtype=np.float32)\n",
        "faiss.normalize_L2(train_embeddings_np)\n",
        "index.train(train_embeddings_np)\n",
        "index.add(train_embeddings_np)\n",
        "index.nprobe = 10\n",
        "print(f\"build_faiss_index took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# compute hybrid similarity for a single query\n",
        "def compute_query_similarities(args):\n",
        "    start_time = time.time()\n",
        "    i, query_emb, query_math, train_embeddings, train_math, docnos = args\n",
        "    query_emb_np = np.ascontiguousarray(query_emb.cpu().numpy(), dtype=np.float32).reshape(1, -1)\n",
        "    if query_emb_np.shape[1] != train_embeddings.shape[1]:\n",
        "        raise ValueError(f\"Query embedding dimension {query_emb_np.shape[1]} does not match index dimension {train_embeddings.shape[1]}\")\n",
        "    faiss.normalize_L2(query_emb_np)\n",
        "    k = 500\n",
        "    distances, indices = index.search(query_emb_np, k)\n",
        "    similarities = []\n",
        "    for j, idx in enumerate(indices[0]):\n",
        "        if idx >= len(docnos):\n",
        "            continue\n",
        "        doc_emb = train_embeddings[idx]\n",
        "        doc_math = train_math[idx]\n",
        "        text_sim = distances[0][j]\n",
        "        math_sim = 0.0\n",
        "        if query_math and doc_math:\n",
        "            math_scores = [math_similarity(qm, dm) for qm in query_math[:1] for dm in doc_math[:1]]\n",
        "            math_sim = max(math_scores) if math_scores else 0.0\n",
        "        sim_score = 0.7 * text_sim + 0.3 * math_sim\n",
        "        similarities.append((docnos[idx], sim_score))\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    print(f\"compute_query_similarities for query {i} took {time.time() - start_time:.2f} seconds\")\n",
        "    return i, similarities[:50]\n",
        "# Parallel similarity computation\n",
        "start_time = time.time()\n",
        "try:\n",
        "    pool = Pool()\n",
        "    args = [(i, query_emb, test_df['math_expressions'][i], train_text_embeddings, train_df['math_expressions'], train_df['docno'])\n",
        "            for i, query_emb in enumerate(test_text_embeddings)]\n",
        "    results_parallel = pool.map(compute_query_similarities, args)\n",
        "finally:\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "print(f\"compute_similarities_parallel took {time.time() - start_time:.2f} seconds\")\n",
        "#generate, collexct and save results\n",
        "start_time = time.time()\n",
        "results = []\n",
        "for i, similarities in sorted(results_parallel, key=lambda x: x[0]):\n",
        "    query_id = test_df['qid'][i]\n",
        "    for docno, sim_score in similarities:\n",
        "        results.append({\n",
        "            'query_ID': query_id,\n",
        "            'retrieved_body_ID': docno,\n",
        "            'Run No.': 1,\n",
        "            'Similarity Score': sim_score\n",
        "        })\n",
        "print(f\"collect_results took {time.time() - start_time:.2f} seconds\")\n",
        "try:\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "    print(f\"Results saved to {output_path}\")\n",
        "    print(f\"Output shape: {results_df.shape}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error saving results to {output_path}: {e}\")"
      ],
      "metadata": {
        "id": "nywT4vIn8pD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58ec123-1dfb-4574-c6e5-b382387371b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (1.13.3)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.55.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy) (1.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n",
            "Pandas imported successfully, version: 2.2.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to mount Google Drive: mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27658439.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27658439.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to mount Google Drive: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#MODEL INITALIZING IS DONE OVER HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to mount Google Drive: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN-02"
      ],
      "metadata": {
        "id": "fnLHWwLx8ssB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required dependencies\n",
        "!pip install pandas numpy sentence-transformers sympy faiss-cpu spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import sympy as sp\n",
        "import re\n",
        "import warnings\n",
        "import torch\n",
        "import faiss\n",
        "import os\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "import spacy\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"Pandas imported successfully, version:\", pd.__version__)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to mount Google Drive: {e}\")\n",
        "\n",
        "# MODEL INITIALIZATION\n",
        "try:\n",
        "    model = SentenceTransformer('all-mpnet-base-v2', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to initialize SentenceTransformer: {e}\")\n",
        "expected_dim = model.get_sentence_embedding_dimension()\n",
        "print(f\"Model expected dimension: {expected_dim}\")\n",
        "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# File paths\n",
        "train_data_path = '/content/drive/MyDrive/clmir/indexed_documents.csv'\n",
        "test_data_path = '/content/drive/MyDrive/clmir/indexed_test_data.csv'\n",
        "train_embeddings_path = '/content/drive/MyDrive/clmir/train_embeddings.csv'\n",
        "output_path = '/content/drive/MyDrive/clmir/run02.csv'\n",
        "visualization_path = '/content/drive/MyDrive/clmir/similarity_visualization.csv'\n",
        "\n",
        "# DEBUGGING OF FILE PATHS\n",
        "print(f\"Train data path: {train_data_path}, Type: {type(train_data_path)}\")\n",
        "print(f\"Test data path: {test_data_path}, Type: {type(test_data_path)}\")\n",
        "print(f\"Train embeddings path: {train_embeddings_path}, Type: {type(train_embeddings_path)}\")\n",
        "print(f\"Output path: {output_path}, Type: {type(output_path)}\")\n",
        "\n",
        "# VALIDATION OF FILE PATH TYPES\n",
        "if not isinstance(train_data_path, (str, bytes, os.PathLike)):\n",
        "    raise TypeError(f\"train_data_path must be a string or PathLike, got {type(train_data_path)}\")\n",
        "if not isinstance(test_data_path, (str, bytes, os.PathLike)):\n",
        "    raise TypeError(f\"test_data_path must be a string or PathLike, got {type(test_data_path)}\")\n",
        "if not isinstance(train_embeddings_path, (str, bytes, os.PathLike)):\n",
        "    raise TypeError(f\"train_embeddings_path must be a string or PathLike, got {type(train_embeddings_path)}\")\n",
        "\n",
        "# CHECKS FOR FILE EXISTENCE\n",
        "if not os.path.exists(train_data_path):\n",
        "    raise FileNotFoundError(f\"Training file not found at {train_data_path}. Ensure the file exists and Google Drive is mounted.\")\n",
        "if not os.path.exists(test_data_path):\n",
        "    raise FileNotFoundError(f\"Test file not found at {test_data_path}. Ensure the file exists and Google Drive is mounted.\")\n",
        "\n",
        "# DATA LOADING\n",
        "start_time = time.time()\n",
        "try:\n",
        "    train_df = pd.read_csv(train_data_path)\n",
        "    test_df = pd.read_csv(test_data_path)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error loading CSV files: {e}\")\n",
        "print(f\"load_data took {time.time() - start_time:.2f} seconds\")\n",
        "print(\"Training data columns:\", train_df.columns.tolist())\n",
        "print(\"Test data columns:\", test_df.columns.tolist())\n",
        "print(f\"Training data size: {len(train_df)} documents\")\n",
        "print(f\"Test data size: {len(test_df)} queries\")\n",
        "\n",
        "# VALIDATE DATASET COLUMNS AND SIZE\n",
        "if 'docno' not in train_df.columns or 'text' not in train_df.columns:\n",
        "    raise KeyError(\"Training data must contain 'docno' and 'text' columns\")\n",
        "if 'qid' not in test_df.columns or 'query' not in test_df.columns:\n",
        "    raise KeyError(\"Test data must contain 'qid' and 'query' columns\")\n",
        "if len(test_df) != 50:\n",
        "    raise ValueError(f\"Test set must contain exactly 50 queries, got {len(test_df)}\")\n",
        "if len(train_df) == 0:\n",
        "    raise ValueError(\"Training dataset is empty\")\n",
        "if len(test_df) == 0:\n",
        "    raise ValueError(\"Test dataset is empty\")\n",
        "\n",
        "# CLEANING LATEX EXPRESSIONS\n",
        "def clean_latex(expr):\n",
        "    expr = re.sub(r'\\\\[a-zA-Z]+', '', expr)\n",
        "    expr = expr.replace('{', '').replace('}', '')\n",
        "    expr = expr.replace('^', '**')\n",
        "    expr = re.sub(r'\\s+', '', expr)\n",
        "    return expr\n",
        "\n",
        "# ENHANCED TEXT AND MATH EXTRACTION WITH SPACY\n",
        "def extract_text_math(content):\n",
        "    content = str(content) if content is not None else ''\n",
        "    doc = nlp(content)\n",
        "    clean_text = ' '.join(token.text for token in doc if not token.text.startswith('$'))\n",
        "    math_pattern = r'\\$.*?\\$|[\\w\\^*/+\\-()=]+|\\sum.*?\\)|integral.*?\\)|partial.*?\\)|limit.*?\\)'\n",
        "    math_expressions = [clean_latex(expr) for expr in re.findall(math_pattern, content)][:1]\n",
        "    return clean_text or ' ', math_expressions\n",
        "\n",
        "# PREPROCESSING DATA\n",
        "start_time = time.time()\n",
        "try:\n",
        "    train_df['clean_text'], train_df['math_expressions'] = zip(*train_df['text'].apply(extract_text_math))\n",
        "    test_df['clean_text'], test_df['math_expressions'] = zip(*test_df['query'].apply(extract_text_math))\n",
        "    train_df['clean_text'] = train_df['clean_text'].replace('', ' ').fillna(' ')\n",
        "    test_df['clean_text'] = test_df['clean_text'].replace('', ' ').fillna(' ')\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error preprocessing data: {e}\")\n",
        "print(f\"preprocess_data took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# CACHED SYMPIFY WITH ENHANCED ERROR HANDLING\n",
        "math_cache = {}\n",
        "def cached_sympify(expr):\n",
        "    if not expr:\n",
        "        return None\n",
        "    if expr not in math_cache:\n",
        "        try:\n",
        "            parsed = sp.sympify(expr, evaluate=False)\n",
        "            simplified = sp.simplify(parsed)\n",
        "            math_cache[expr] = simplified\n",
        "        except:\n",
        "            math_cache[expr] = None\n",
        "    return math_cache[expr]\n",
        "\n",
        "# ENHANCED MATH SIMILARITY\n",
        "def math_similarity(expr1, expr2):\n",
        "    try:\n",
        "        parsed_expr1 = cached_sympify(expr1)\n",
        "        parsed_expr2 = cached_sympify(expr2)\n",
        "        if parsed_expr1 is None or parsed_expr2 is None:\n",
        "            return 0.0\n",
        "        if str(parsed_expr1) == str(parsed_expr2):\n",
        "            return 1.0\n",
        "        diff = sp.simplify(parsed_expr1 - parsed_expr2)\n",
        "        if diff == 0:\n",
        "            return 1.0\n",
        "        try:\n",
        "            x = sp.Symbol('x')\n",
        "            eval_diff = abs(float(parsed_expr1.subs(x, 1) - parsed_expr2.subs(x, 1)))\n",
        "            return max(0.0, 1.0 - eval_diff / (abs(float(parsed_expr1.subs(x, 1))) + 1e-10))\n",
        "        except:\n",
        "            return 0.5\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "# LOAD OR COMPUTE EMBEDDINGS WITH MIXED PRECISION\n",
        "start_time = time.time()\n",
        "batch_size = 128 if torch.cuda.is_available() else 64  # Reduced batch size to avoid OOM\n",
        "if os.path.exists(train_embeddings_path):\n",
        "    print(\"Loading precomputed train embeddings...\")\n",
        "    try:\n",
        "        embeddings_df = pd.read_csv(train_embeddings_path)\n",
        "        print(f\"Precomputed embeddings CSV shape: {embeddings_df.shape}\")\n",
        "        if embeddings_df.shape[1] != expected_dim:\n",
        "            print(f\"Warning: Precomputed embeddings dimension {embeddings_df.shape[1]} does not match model dimension {expected_dim}. Recomputing embeddings...\")\n",
        "            with autocast():\n",
        "                train_text_embeddings = model.encode(train_df['clean_text'].tolist(), batch_size=batch_size, convert_to_tensor=True, show_progress_bar=True)\n",
        "            pd.DataFrame(train_text_embeddings.cpu().numpy()).to_csv(train_embeddings_path, index=False)\n",
        "            print(f\"Saved new train embeddings to {train_embeddings_path}, shape: {train_text_embeddings.shape}\")\n",
        "        else:\n",
        "            train_text_embeddings = torch.tensor(embeddings_df.values, dtype=torch.float32).to(model.device)\n",
        "            print(f\"Loaded train embeddings shape: {train_text_embeddings.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading embeddings: {e}. Recomputing embeddings...\")\n",
        "        with autocast():\n",
        "            train_text_embeddings = model.encode(train_df['clean_text'].tolist(), batch_size=batch_size, convert_to_tensor=True, show_progress_bar=True)\n",
        "        pd.DataFrame(train_text_embeddings.cpu().numpy()).to_csv(train_embeddings_path, index=False)\n",
        "        print(f\"Saved new train embeddings to {train_embeddings_path}, shape: {train_text_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"Computing train embeddings...\")\n",
        "    with autocast():\n",
        "        train_text_embeddings = model.encode(train_df['clean_text'].tolist(), batch_size=batch_size, convert_to_tensor=True, show_progress_bar=True)\n",
        "    pd.DataFrame(train_text_embeddings.cpu().numpy()).to_csv(train_embeddings_path, index=False)\n",
        "    print(f\"Saved new train embeddings to {train_embeddings_path}, shape: {train_text_embeddings.shape}\")\n",
        "\n",
        "try:\n",
        "    with autocast():\n",
        "        test_text_embeddings = model.encode(test_df['clean_text'].tolist(), batch_size=batch_size, convert_to_tensor=True, show_progress_bar=True)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error computing test embeddings: {e}\")\n",
        "print(f\"Test embeddings shape: {test_text_embeddings.shape}\")\n",
        "print(f\"load_or_compute_embeddings took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# DIMENSIONALITY VERIFICATION\n",
        "if train_text_embeddings.shape[1] != expected_dim:\n",
        "    raise ValueError(f\"Train embeddings dimension {train_text_embeddings.shape[1]} does not match model dimension {expected_dim}\")\n",
        "if test_text_embeddings.shape[1] != expected_dim:\n",
        "    raise ValueError(f\"Test embeddings dimension {test_text_embeddings.shape[1]} does not match model dimension {expected_dim}\")\n",
        "\n",
        "# BUILDING FAISS INDEX\n",
        "start_time = time.time()\n",
        "dimension = train_text_embeddings.shape[1]\n",
        "nlist = min(200, max(1, len(train_df) // 10))  # Ensure nlist is at least 1\n",
        "quantizer = faiss.IndexFlatIP(dimension)\n",
        "index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
        "train_embeddings_np = np.ascontiguousarray(train_text_embeddings.cpu().numpy(), dtype=np.float32)\n",
        "faiss.normalize_L2(train_embeddings_np)\n",
        "try:\n",
        "    index.train(train_embeddings_np)\n",
        "    index.add(train_embeddings_np)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error building FAISS index: {e}\")\n",
        "index.nprobe = min(20, nlist)  # Dynamic nprobe\n",
        "print(f\"build_faiss_index took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# COMPUTE HYBRID SIMILARITY FOR A SINGLE QUERY\n",
        "def compute_query_similarities(args):\n",
        "    start_time = time.time()\n",
        "    i, query_emb, query_math, train_embeddings, train_math, docnos = args\n",
        "    query_emb_np = np.ascontiguousarray(query_emb.cpu().numpy(), dtype=np.float32).reshape(1, -1)\n",
        "    if query_emb_np.shape[1] != train_embeddings.shape[1]:\n",
        "        raise ValueError(f\"Query embedding dimension {query_emb_np.shape[1]} does not match index dimension {train_embeddings.shape[1]}\")\n",
        "    faiss.normalize_L2(query_emb_np)\n",
        "    k = 500\n",
        "    try:\n",
        "        distances, indices = index.search(query_emb_np, k)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error during FAISS search for query {i}: {e}\")\n",
        "    similarities = []\n",
        "    for j, idx in enumerate(indices[0]):\n",
        "        if idx >= len(docnos):\n",
        "            continue\n",
        "        doc_emb = train_embeddings[idx]\n",
        "        doc_math = train_math[idx]\n",
        "        text_sim = distances[0][j]\n",
        "        math_sim = 0.0\n",
        "        if query_math and doc_math:\n",
        "            math_scores = [math_similarity(qm, dm) for qm in query_math[:1] for dm in doc_math[:1]]\n",
        "            math_sim = max(math_scores) if math_scores else 0.0\n",
        "        sim_score = 0.6 * text_sim + 0.4 * math_sim  # Adjusted weights\n",
        "        similarities.append((docnos[idx], sim_score, text_sim, math_sim))\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    print(f\"compute_query_similarities for query {i} took {time.time() - start_time:.2f} seconds\")\n",
        "    return i, similarities[:50]\n",
        "\n",
        "# PARALLEL SIMILARITY COMPUTATION\n",
        "start_time = time.time()\n",
        "try:\n",
        "    pool = Pool()\n",
        "    args = [(i, query_emb, test_df['math_expressions'][i], train_text_embeddings, train_df['math_expressions'], train_df['docno'])\n",
        "            for i, query_emb in enumerate(test_text_embeddings)]\n",
        "    results_parallel = pool.map(compute_query_similarities, args)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error in parallel similarity computation: {e}\")\n",
        "finally:\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "print(f\"compute_similarities_parallel took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# COLLECT AND SAVE RESULTS\n",
        "start_time = time.time()\n",
        "results = []\n",
        "visualization_data = []\n",
        "for i, similarities in sorted(results_parallel, key=lambda x: x[0]):\n",
        "    query_id = test_df['qid'][i]\n",
        "    for rank, (docno, sim_score, text_sim, math_sim) in enumerate(similarities, 1):\n",
        "        results.append({\n",
        "            'query_ID': query_id,\n",
        "            'retrieved_body_ID': docno,\n",
        "            'Run No.': 2,\n",
        "            'Similarity Score': sim_score\n",
        "        })\n",
        "        visualization_data.append({\n",
        "            'query_ID': query_id,\n",
        "            'docno': docno,\n",
        "            'rank': rank,\n",
        "            'text_similarity': text_sim,\n",
        "            'math_similarity': math_sim,\n",
        "            'combined_similarity': sim_score\n",
        "        })\n",
        "print(f\"collect_results took {time.time() - start_time:.2f} seconds\")\n",
        "try:\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "    print(f\"Results saved to {output_path}, shape: {results_df.shape}\")\n",
        "    vis_df = pd.DataFrame(visualization_data)\n",
        "    vis_df.to_csv(visualization_path, index=False)\n",
        "    print(f\"Visualization data saved to {visualization_path}, shape: {vis_df.shape}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error saving results to {output_path}: {e}\")"
      ],
      "metadata": {
        "id": "pYN0piip8vFW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}